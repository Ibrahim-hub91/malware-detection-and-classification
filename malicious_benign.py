# -*- coding: utf-8 -*-
"""Malicious Benign.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UqJRmR_o4o7p4saaenLOy_VMB7n9kQLi

# Preprocessing
"""

from google.colab import drive
drive.mount("/content/drive")

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/MyDrive/projects/malicious benign"

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.manifold import TSNE
from joblib import dump, load

df = pd.read_csv("benign_malicious samples.csv")
df.head()

df.isnull().sum()

df = df.dropna()
df.shape

df['category'] = df['class'].apply(lambda x: 'Benign' if x == 'Benign' else 'Malicious')

"""# Exploratory Data Analysis

The bar plot visualizes the percentage of the total that each category i.e. malicious and benign represents, which can give you a sense of whether the classes are balanced or imbalanced. If one category represents a much larger percentage of the total than the other, you have class imbalance, which can affect the performance of machine learning models.
"""

# Calculate the total number of entries in the DataFrame
total_count = len(df)

# Count the occurrences of each category in the 'category' column
category_counts = df['category'].value_counts()

# Calculate the percentage of each category by dividing the count by the total and multiplying by 100
percentages = (category_counts / total_count) * 100

# Create a bar plot using the percentages as the heights and category names as the x-axis labels
plt.bar(percentages.index, percentages.values)

# Add labels to the axes and a title to the plot
plt.xlabel('Categories')
plt.ylabel('Percentage')
plt.title('Percentage of Malicious and Benign Category')

# Add labels with the percentage values on top of each bar
for i, percentage in enumerate(percentages.values):
    plt.text(i, percentage, f"{percentage:.2f}%", ha='center', va='bottom')

# Adjust the layout and display the plot
plt.tight_layout()
plt.show()

"""The bar plot visualizes the percentage of the total that each classes i.e. benign and other malware classes. This will allow us to identify the total percentage of each class in our sample data."""

# Calculate the total number of entries in the DataFrame
total_count = len(df)

# Count the occurrences of each class in the 'class' column
category_counts = df['class'].value_counts()

# Calculate the percentage of each class by dividing the count by the total and multiplying by 100
percentages = (category_counts / total_count) * 100

# Create a bar plot using the percentages as the heights and class names as the x-axis labels
plt.bar(percentages.index, percentages.values)

# Add labels to the axes and a title to the plot
plt.xlabel('Categories')
plt.ylabel('Percentage')
plt.title('Percentage of Each Class')

# Add labels with the percentage values on top of each bar
for i, percentage in enumerate(percentages.values):
    plt.text(i, percentage, f"{percentage:.2f}%", ha='center', va='bottom')

# Adjust the layout and display the plot
plt.tight_layout()
plt.show()

"""Since approximately 50% of the data belongs to benign class, we will now visualize the percentage of other malicious samples."""

# Set the size of the figure
plt.figure(figsize=(23, 8))

# Filter the DataFrame to include only rows with the category 'Malicious'
df_mal = df[df['category'] == 'Malicious']

# Calculate the total number of entries in the filtered DataFrame
total_count = len(df_mal)

# Count the occurrences of each class in the 'class' column of the filtered DataFrame
category_counts = df_mal['class'].value_counts()

# Calculate the percentage of each class within the 'Malicious' category
percentages = (category_counts / total_count) * 100

# Create a bar plot using the percentages as the heights and class names as the x-axis labels
plt.bar(percentages.index, percentages.values)

# Add labels to the axes and a title to the plot
plt.xlabel('Categories')
plt.ylabel('Percentage')
plt.title('Percentage of Each Malicious Category')

# Add labels with the percentage values on top of each bar
for i, percentage in enumerate(percentages.values):
    plt.text(i, percentage, f"{percentage:.2f}%", ha='center', va='bottom')

# Adjust the layout and display the plot
plt.tight_layout()
plt.show()

"""From now on, we will calucate variance of our data features before visualization. Variance is a statistical measurement that describes the spread of data points in a data distribution. It measures how far each number in the set is from the mean (average) and thus from every other number in the set. Variance is a useful tool to understand which features are more likely to be significant. Features with higher variance may contain more useful information, while features with very low variance might not contribute significantly to our model's predictive power because they don't vary much within the data."""

# Calculate the variances of features in the DataFrame, excluding columns 'id', 'class', and 'category'
variances = df.drop(['id', 'class', 'category'], axis=1).var().sort_values(ascending=False)

# Select the top 10 features with the highest variances
top_features = variances.head(10).index

# Set the size of the figure for the heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(df[list(top_features)].corr(), annot=True, fmt=".2f", cmap='coolwarm', center=0)

# Set the title for the plot
plt.title('Heatmap for Top 10 Features')

# Display the heatmap plot
plt.show()

"""Now, let's visualize the data of each category i.e. benign and malicious. Since data is categorical, we will visualize the count plot of top 10 features of each category."""

# Calculate the variances of features in the DataFrame, excluding columns 'id' and 'class'
variances = df.drop(['id', 'class'], axis=1).var().sort_values(ascending=False)

# Select the top 10 features with the highest variances
top_features = variances.head(10).index

# Create subplots for the box plots with a 5x2 grid (5 rows, 2 columns) and set the figure size
fig, axes = plt.subplots(5, 2, figsize=(15, 20))

# Loop through each of the top 10 features and create a count plot for each feature
for i, feature in enumerate(top_features):
    # Use Seaborn's countplot to display the distribution of each category (e.g., 'Malicious' and 'Benign')
    # for the current feature on the subplot corresponding to its position in the grid
    sns.countplot(data=df, x=feature, hue='category', ax=axes[i // 2, i % 2])

    # Set a title for each count plot, indicating the feature name
    axes[i // 2, i % 2].set_title(f'Count Plot for {feature}')

# Adjust the layout to prevent overlapping of subplots
fig.tight_layout()

# Display the subplots
plt.show()

"""Let's visualize the data distribution of each malicious class."""

# Filter the DataFrame to include only rows with the category 'Malicious'
df_mal = df[df['category'] == 'Malicious']

# Calculate the variances of features in the 'Malicious' DataFrame, excluding columns 'id' and 'category'
variances = df_mal.drop(['id', 'category'], axis=1).var().sort_values(ascending=False)

# Select the top 10 features with the highest variances
top_features = variances.head(10).index

# Create subplots for the count plots with 10 rows and 1 column and set the figure size
fig, axes = plt.subplots(nrows=10, ncols=1, figsize=(15, 40))

# Loop through each of the top 10 features and create a count plot for each feature
for i, feature in enumerate(top_features):
    # Use Seaborn's countplot to display the distribution of each class ('0' and '1') for the current feature
    # on the subplot corresponding to its position in the grid. Here, 'y=feature' changes the orientation
    # of the count plot to horizontal, making it easier to visualize the feature names on the y-axis.
    sns.countplot(data=df_mal, y=feature, hue='class', ax=axes[i], orient='h')

    # Set a title for each count plot, indicating the feature name
    axes[i].set_title(f'Count Plot for {feature}')

# Adjust the layout to prevent overlapping of subplots
fig.tight_layout()

# Display the subplots
plt.show()

"""t-Distributed Stochastic Neighbor Embedding (t-SNE) is a powerful machine learning algorithm used for visualization and dimensionality reduction, particularly effective at mapping high-dimensional data into two or three dimensions. By calculating the similarity of points in high-dimensional space and adjusting their representation in the low-dimensional space, it allows for visualization of complex data structures, revealing clusters and patterns that might be difficult to spot otherwise. Though typically not applied directly to categorical data, t-SNE can be used after transforming such data into numerical form. It's especially valuable for exploratory data analysis, providing insights into the organization and relationships within the dataset."""

# Calculate the variances of features in the DataFrame, excluding columns 'id' and 'class'
variances = df.drop(['id', 'class'], axis=1).var().sort_values(ascending=False)

# Select the top 10 features with the highest variances
top_features = variances.head(10).index

# Create a subset DataFrame with only the top 10 features
df_subset = df[list(top_features)]

# Assign the features (X_subset) and the target variable (y_subset) for the t-SNE plot
X_subset = df_subset
y_subset = df['category']

# Encode the class labels as integers for the scatter plot using LabelEncoder
le = LabelEncoder()
y_subset_encoded = le.fit_transform(y_subset)

# Perform t-SNE dimensionality reduction to reduce the features to 2 dimensions
tsne = TSNE(n_components=2, random_state=0)
X_subset_tsne = tsne.fit_transform(X_subset)

# Create the t-SNE scatter plot
plt.figure(figsize=(10, 8))

# Use scatter plot to visualize the data points based on the t-SNE transformed features
# 'c=y_subset_encoded' assigns different colors to data points based on their encoded class labels
# 'cmap='coolwarm'' sets the color map for the scatter plot to 'coolwarm' (blue for one class, red for the other)
# 'alpha=0.6' adjusts the transparency of the data points for better visualization
scatter = plt.scatter(X_subset_tsne[:, 0], X_subset_tsne[:, 1], c=y_subset_encoded, cmap='coolwarm', alpha=0.6)

# Set a title for the scatter plot
plt.title('t-SNE Plot of the Dataset')

# Create a colorbar to display the original class labels on the plot
cbar = plt.colorbar(scatter, ticks=range(len(le.classes_)))
cbar.set_label('class')

# Use the original class labels from the LabelEncoder to display on the colorbar
cbar.set_ticklabels(le.classes_)

# Display the t-SNE scatter plot
plt.show()

# Calculate the variances of features in the DataFrame, excluding columns 'id' and 'class'
variances = df.drop(['id', 'class'], axis=1).var().sort_values(ascending=False)

# Select the top 10 features with the highest variances
top_features = variances.head(10).index

# Create a subset DataFrame with only the top 10 features
df_subset = df[list(top_features)]

# Assign the features (X_subset) and the target variable (y_subset) for the t-SNE plot
X_subset = df_subset
y_subset = df['class']

# Encode the class labels as integers for the scatter plot using LabelEncoder
le = LabelEncoder()
y_subset_encoded = le.fit_transform(y_subset)

# Perform t-SNE dimensionality reduction to reduce the features to 2 dimensions
tsne = TSNE(n_components=2, random_state=0)
X_subset_tsne = tsne.fit_transform(X_subset)

# Create the t-SNE scatter plot
plt.figure(figsize=(10, 8))

# Use scatter plot to visualize the data points based on the t-SNE transformed features
# 'c=y_subset_encoded' assigns different colors to data points based on their encoded class labels
# 'cmap='coolwarm'' sets the color map for the scatter plot to 'coolwarm' (blue for one class, red for the other)
# 'alpha=0.6' adjusts the transparency of the data points for better visualization
scatter = plt.scatter(X_subset_tsne[:, 0], X_subset_tsne[:, 1], c=y_subset_encoded, cmap='coolwarm', alpha=0.6)

# Set a title for the scatter plot
plt.title('t-SNE Plot of the Dataset for each class')

# Create a colorbar to display the original class labels on the plot
cbar = plt.colorbar(scatter, ticks=range(len(le.classes_)))
cbar.set_label('class')

# Use the original class labels from the LabelEncoder to display on the colorbar
cbar.set_ticklabels(le.classes_)

# Display the t-SNE scatter plot
plt.show()



"""# Classifier

## Malicious VS Benign
"""

df_mal_ben = df.copy()
df_mal_ben.drop(columns=["class", 'id'], inplace=True)

le = LabelEncoder()
y = df_mal_ben['category']
y = le.fit_transform(y)

x = df_mal_ben[df_mal_ben.columns[:-1]]

x = np.array(x)
y = np.array(y)

# splitting with 70% training and 30% test data
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, stratify=y, random_state=42)

rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the classifier on the training data
rf_classifier.fit(X_train, y_train)

# Validate the model
y_pred = rf_classifier.predict(X_test)

valid_accuracy = accuracy_score(y_test, y_pred)
print("Validation Accuracy:", valid_accuracy)
valid_precision = precision_score(y_test, y_pred, average='micro')
print("Validation Precision:", valid_precision)
valid_recall = recall_score(y_test, y_pred, average='micro')
print("Validation Recall:", valid_recall)
valid_f1 = f1_score(y_test, y_pred, average='micro')
print("Validation Recall:", valid_f1)
report = classification_report(y_test, y_pred)
print(f"Classification Report: \n{report}")
cm = confusion_matrix(y_test, y_pred)
print("CM:,", cm)
labels = le.inverse_transform([0,1])
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues',
            xticklabels=labels, yticklabels=labels)
plt.title("Confusion for Heart Disease Prediction for Validation Dataset")
plt.xlabel('Predicted labels')
plt.ylabel('True labels')

class_accuracy = {label: np.mean((y_test == label) == (y_pred == label)) for label in set(df['category'])}
# Create lists of class names and corresponding accuracy rates
classes = list(class_accuracy.keys())
accuracy_rates = list(class_accuracy.values())

# Create a bar chart
bars = plt.bar(classes, accuracy_rates, color='lightcoral')

# Label the axes
plt.xlabel('Classes')
plt.ylabel('Accuracy Rate')

# Title the chart
plt.title('Accuracy Rate for Each Class')

# Add accuracy rate on top of each bar
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, yval, round(yval, 2), va='bottom')  # va: vertical alignment

# Display the chart
plt.tight_layout()
plt.show()

dump(rf_classifier, './malicious benign/model.pkl')

"""**Applying grid search**"""

# Grid Search

# Define the parameter grid
param_grid = {
    'n_estimators': [5, 10, 50, 100, 150],
    'max_depth': [1, 5, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize the Random Forest Classifier
rf_classifier = RandomForestClassifier()

# Create a GridSearchCV instance
grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5)

# Perform grid search on the training data
grid_search.fit(X_train, y_train)

# Print the best hyperparameters and the best score
print("Best Hyperparameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)

rf_classifier = RandomForestClassifier(max_depth= 10,
                                       min_samples_leaf= 1,
                                       min_samples_split= 2,
                                       n_estimators= 10)

# Train the classifier on the training data
rf_classifier.fit(X_train, y_train)

y_pred = rf_classifier.predict(X_test)

valid_accuracy = accuracy_score(y_test, y_pred)
print("Validation Accuracy:", valid_accuracy)
valid_precision = precision_score(y_test, y_pred, average='micro')
print("Validation Precision:", valid_precision)
valid_recall = recall_score(y_test, y_pred, average='micro')
print("Validation Recall:", valid_recall)
valid_f1 = f1_score(y_test, y_pred, average='micro')
print("Validation Recall:", valid_f1)
report = classification_report(y_test, y_pred)
print(f"Classification Report: \n{report}")
cm = confusion_matrix(y_test, y_pred)
print("CM:,", cm)
labels = le.inverse_transform([0,1])
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues',
            xticklabels=labels, yticklabels=labels)
plt.title("Confusion for Heart Disease Prediction for Validation Dataset")
plt.xlabel('Predicted labels')
plt.ylabel('True labels')

class_accuracy = {label: np.mean((y_test == label) == (y_pred == label)) for label in set(df['category'])}
# Create lists of class names and corresponding accuracy rates
classes = list(class_accuracy.keys())
accuracy_rates = list(class_accuracy.values())

# Create a bar chart
bars = plt.bar(classes, accuracy_rates, color='lightcoral')

# Label the axes
plt.xlabel('Classes')
plt.ylabel('Accuracy Rate')

# Title the chart
plt.title('Accuracy Rate for Each Class')

# Add accuracy rate on top of each bar
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, yval, round(yval, 2), va='bottom')  # va: vertical alignment

# Display the chart
plt.tight_layout()
plt.show()

"""**Classification with XGBoost**"""

xgb_classifier = xgb.XGBClassifier()

# Train the classifier
xgb_classifier.fit(X_train, y_train)

y_pred = xgb_classifier.predict(X_test)

valid_accuracy = accuracy_score(y_test, y_pred)
print("Validation Accuracy:", valid_accuracy)
valid_precision = precision_score(y_test, y_pred, average='micro')
print("Validation Precision:", valid_precision)
valid_recall = recall_score(y_test, y_pred, average='micro')
print("Validation Recall:", valid_recall)
valid_f1 = f1_score(y_test, y_pred, average='micro')
print("Validation Recall:", valid_f1)
report = classification_report(y_test, y_pred)
print(f"Classification Report: \n{report}")
cm = confusion_matrix(y_test, y_pred)
print("CM:,", cm)
labels = le.inverse_transform([0,1])
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues',
            xticklabels=labels, yticklabels=labels)
plt.title("Confusion for Heart Disease Prediction for Validation Dataset")
plt.xlabel('Predicted labels')
plt.ylabel('True labels')

class_accuracy = {label: np.mean((y_test == label) == (y_pred == label)) for label in set(df['category'])}
# Create lists of class names and corresponding accuracy rates
classes = list(class_accuracy.keys())
accuracy_rates = list(class_accuracy.values())

# Create a bar chart
bars = plt.bar(classes, accuracy_rates, color='lightcoral')

# Label the axes
plt.xlabel('Classes')
plt.ylabel('Accuracy Rate')

# Title the chart
plt.title('Accuracy Rate for Each Class')

# Add accuracy rate on top of each bar
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, yval, round(yval, 2), va='bottom')  # va: vertical alignment

# Display the chart
plt.tight_layout()
plt.show()

"""## All Samples Classification"""

df_class = df.copy()
y = df_class['class']
df_class.drop(columns=['class', 'category', 'id'], inplace=True)

x = np.array(df_class)
y = le.fit_transform(y)
y = np.array(y)

# splitting with 70% training and 30% test data
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, stratify=y, random_state=42)

rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the classifier on the training data
rf_classifier.fit(X_train, y_train)

plt.figure(figsize=(10,10))
# Validate the model
y_pred = rf_classifier.predict(X_test)

valid_accuracy = accuracy_score(y_test, y_pred)
print("Validation Accuracy:", valid_accuracy)
valid_precision = precision_score(y_test, y_pred, average='micro')
print("Validation Precision:", valid_precision)
valid_recall = recall_score(y_test, y_pred, average='micro')
print("Validation Recall:", valid_recall)
valid_f1 = f1_score(y_test, y_pred, average='micro')
print("Validation Recall:", valid_f1)
report = classification_report(y_test, y_pred)
print(f"Classification Report: \n{report}")
cm = confusion_matrix(y_test, y_pred)
print("CM:,", cm)
values = [i for i in range(0, len(set(df['class'])))]
labels = le.inverse_transform(values)
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues',
            xticklabels=labels, yticklabels=labels)
plt.title("Confusion for Heart Disease Prediction for Validation Dataset")
plt.xlabel('Predicted labels')
plt.ylabel('True labels')

plt.figure(figsize=(23,8))
class_accuracy = {label: np.mean((y_test == label) == (y_pred == label)) for label in set(df['class'])}
# Create lists of class names and corresponding accuracy rates
classes = list(class_accuracy.keys())
accuracy_rates = list(class_accuracy.values())

# Create a bar chart
bars = plt.bar(classes, accuracy_rates, color='lightcoral')

# Label the axes
plt.xlabel('Classes')
plt.ylabel('Accuracy Rate')

# Title the chart
plt.title('Accuracy Rate for Each Class')

# Add accuracy rate on top of each bar
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, yval, round(yval, 2), va='bottom')  # va: vertical alignment

# Display the chart
plt.tight_layout()
plt.show()

dump(rf_classifier, './all classes/model.pkl')

"""**Applying Grid Search**"""

# Define the parameter grid
param_grid = {
    'n_estimators': [5, 10, 50, 100, 150],
    'max_depth': [1, 5, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize the Random Forest Classifier
rf_classifier = RandomForestClassifier()

# Create a GridSearchCV instance
grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5)

# Perform grid search on the training data
grid_search.fit(X_train, y_train)

# Print the best hyperparameters and the best score
print("Best Hyperparameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)

rf_classifier = RandomForestClassifier(max_depth= 10, min_samples_leaf= 2,
                                       min_samples_split = 2, n_estimators = 150)

# Train the classifier on the training data
rf_classifier.fit(X_train, y_train)

plt.figure(figsize=(10,10))
# Validate the model
y_pred = rf_classifier.predict(X_test)

valid_accuracy = accuracy_score(y_test, y_pred)
print("Validation Accuracy:", valid_accuracy)
valid_precision = precision_score(y_test, y_pred, average='micro')
print("Validation Precision:", valid_precision)
valid_recall = recall_score(y_test, y_pred, average='micro')
print("Validation Recall:", valid_recall)
valid_f1 = f1_score(y_test, y_pred, average='micro')
print("Validation Recall:", valid_f1)
report = classification_report(y_test, y_pred)
print(f"Classification Report: \n{report}")
cm = confusion_matrix(y_test, y_pred)
print("CM:,", cm)
values = [i for i in range(0, len(set(df['class'])))]
labels = le.inverse_transform(values)
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues',
            xticklabels=labels, yticklabels=labels)
plt.title("Confusion for Heart Disease Prediction for Validation Dataset")
plt.xlabel('Predicted labels')
plt.ylabel('True labels')

plt.figure(figsize=(23,8))
class_accuracy = {label: np.mean((y_test == label) == (y_pred == label)) for label in set(df['class'])}
# Create lists of class names and corresponding accuracy rates
classes = list(class_accuracy.keys())
accuracy_rates = list(class_accuracy.values())

# Create a bar chart
bars = plt.bar(classes, accuracy_rates, color='lightcoral')

# Label the axes
plt.xlabel('Classes')
plt.ylabel('Accuracy Rate')

# Title the chart
plt.title('Accuracy Rate for Each Class')

# Add accuracy rate on top of each bar
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, yval, round(yval, 2), va='bottom')  # va: vertical alignment

# Display the chart
plt.tight_layout()
plt.show()

"""**Classification with XGBoost**"""

xgb_classifier = xgb.XGBClassifier()

# Train the classifier
xgb_classifier.fit(X_train, y_train)

plt.figure(figsize=(10,10))
# Validate the model
y_pred = xgb_classifier.predict(X_test)

valid_accuracy = accuracy_score(y_test, y_pred)
print("Validation Accuracy:", valid_accuracy)
valid_precision = precision_score(y_test, y_pred, average='micro')
print("Validation Precision:", valid_precision)
valid_recall = recall_score(y_test, y_pred, average='micro')
print("Validation Recall:", valid_recall)
valid_f1 = f1_score(y_test, y_pred, average='micro')
print("Validation Recall:", valid_f1)
report = classification_report(y_test, y_pred)
print(f"Classification Report: \n{report}")
cm = confusion_matrix(y_test, y_pred)
print("CM:,", cm)
values = [i for i in range(0, len(set(df['class'])))]
labels = le.inverse_transform(values)
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues',
            xticklabels=labels, yticklabels=labels)
plt.title("Confusion for Heart Disease Prediction for Validation Dataset")
plt.xlabel('Predicted labels')
plt.ylabel('True labels')

plt.figure(figsize=(23,8))
class_accuracy = {label: np.mean((y_test == label) == (y_pred == label)) for label in set(df['class'])}
# Create lists of class names and corresponding accuracy rates
classes = list(class_accuracy.keys())
accuracy_rates = list(class_accuracy.values())

# Create a bar chart
bars = plt.bar(classes, accuracy_rates, color='lightcoral')

# Label the axes
plt.xlabel('Classes')
plt.ylabel('Accuracy Rate')

# Title the chart
plt.title('Accuracy Rate for Each Class')

# Add accuracy rate on top of each bar
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, yval, round(yval, 2), va='bottom')  # va: vertical alignment

# Display the chart
plt.tight_layout()
plt.show()



"""# Malicious Benign User Input Classifier"""

"""
This code will accept features and replace the indices of list initialized
with zeros to 1 depending upon the feature name entered. So for example "Malicious"
sample has following features which are set to 1 in given dataframe:

getprocaddress, writeprocessmemory, sizeofresource, readprocessmemory, lockresource,
createprocessa, loadlibrarya, loadresource, virtualallocex, findresourcea,
findresourcea, getcurrentthreadid, getmodulehandlea, getsystemdirectorya,
freelibrary, getstartupinfoa, getcommandlinea

The program prompts the user to enter feature names one by one until they decide
to stop by typing "exit." As the user enters each feature, the code sets the
corresponding index in the user_input list to 1. This process converts the user's
selected features into a binary representation, where 1 indicates the presence
of a particular feature, and 0 indicates the absence. After naming all these
features user will write "exit" to exit from user input
fields and to predict the output
"""


model = load("./malicious benign/model.pkl")

mal_ben_cols = dict()
user_values = []
user_input = [0 for i in range(115)]
for index, i in enumerate(list(df_mal_ben.columns)[:-1]):
  mal_ben_cols[i] = index

while True:
  try:
      # Get the index and value from the user
      feature = input("Enter the feature: ")
      if feature.lower() == 'exit':
        break
      else:
        user_values.append(feature)

  except IndexError:
      print("Invalid value")

  except ValueError:
      print("Invalid input! Please enter a valid index (an integer).")
for i in set(user_values):
  index = mal_ben_cols.get(i)
  user_input[index] = 1

# Validate user input
if len(user_input) > 115:
    print("Error: Input length should not exceed 115.")

else:
    # Convert the string to list of integers
    input_data = [int(digit) for digit in user_input]

    # Ensure the input data is 2D as the model expects 2D array as input
    input_data = [input_data]

    # Make a prediction
    prediction = model.predict(input_data)
    if prediction[0] == 0:
      pred = "Benign"
    else:
      pred = "Malicious"
    print("The model prediction is:", pred)

"""
# Benign and Sub-classes Classification"""

"""
This code will accept features and replace the indices of list initialized
with zeros to 1 depending upon the feature name entered. So for example "Backdoor"
has following features which are set to 1 in given dataframe:

getprocaddress, writeprocessmemory, sizeofresource, readprocessmemory, lockresource,
createprocessa, loadlibrarya, loadresource, virtualallocex, findresourcea,
findresourcea, getcurrentthreadid, getmodulehandlea, getsystemdirectorya,
freelibrary, getstartupinfoa, getcommandlinea

The program prompts the user to enter feature names one by one until they decide
to stop by typing "exit." As the user enters each feature, the code sets the
corresponding index in the user_input list to 1. This process converts the user's
selected features into a binary representation, where 1 indicates the presence
of a particular feature, and 0 indicates the absence. After naming all these
features user will write "exit" to exit from user input
fields and to predict the output
"""

model = load("./all classes/model.pkl")
mapping ={0: 'AdWare', 1: 'Backdoor', 2: 'Benign', 3: 'Email-Worm', 4: 'Generic Malware', 5: 'Hoax', 6: 'Packed', 7: 'Trojan',
          8: 'Trojan-Downloader', 9: 'Trojan-Dropper', 10: 'Trojan-FakeAV', 11: 'Trojan-GameThief',
          12: 'Trojan-PSW', 13: 'Trojan-Ransom', 14: 'Trojan-Spy', 15: 'Virus', 16: 'Worm'}


mal_ben_cols = dict()
user_values = []
user_input = [0 for i in range(115)]
for index, i in enumerate(list(df_class.columns)):
  mal_ben_cols[i] = index

while True:
  try:
      # Get the index and value from the user
      feature = input("Enter the feature: ")
      if feature.lower() == 'exit':
        break
      else:
        user_values.append(feature)

  except IndexError:
      print("Invalid value")

  except ValueError:
      print("Invalid input! Please enter a valid index (an integer).")
for i in set(user_values):
  index = mal_ben_cols.get(i)
  user_input[index] = 1

# Validate user input
if len(user_input) > 115:
    print("Error: Input length should not exceed 115.")

else:
    # Convert the string to list of integers
    input_data = [int(digit) for digit in user_input]

    # Ensure the input data is 2D as the model expects 2D array as input
    input_data = [input_data]

    # Make a prediction
    prediction = model.predict(input_data)
    pred = mapping.get(prediction[0], None)
    if pred:
        print("The model prediction is:", pred)
    else:
      print("Something went wrong while predicting")



"""## Methodology and results discussion
The data analysis process consisted of different stages. First, the dataset was loaded into the analysis environment, through pandas. Next, data cleaning took place, addressing missing values to ensure data integrity.

Following data cleaning, exploratory data analysis (EDA) was conducted. Through visualization and summary statistics, researchers gained valuable insights into the dataset's structure and distribution. EDA helped identify patterns, correlations, and potential trends among variables, guiding the selection of appropriate analytical methods. Finally, the dataset was split into two subsets: a training set, used to train machine learning models, and a validation set, employed to assess model performance and generalization capabilities.

Initially, we calculate our validation results on binary classes i.e. malicious and benign. The Random Forest model achieved outstanding results with an impressive validation accuracy, precision, recall, and F1-score, all reaching an almost perfect value of approximately 0.9996. The model performed exceptionally well in distinguishing between the two classes, malicious and benign, with minimal misclassifications. This is evident from the confusion matrix, which shows only 5 instances of false negatives (malicious instances wrongly classified as benign) and no false positives (benign instances wrongly classified as malicious) out of a total of 11925 samples. The macro average and weighted average for precision, recall, and F1-score are also 1.00, indicating a well-balanced performance across both classes. The XGBoost model demonstrates exceptional performance, achieving a validation accuracy, precision, recall, and F1-score of approximately 0.9996, which is nearly perfect. Similar to the Random Forest model, XGBoost excels in accurately differentiating between the two classes, malicious and benign, with only 5 false negatives and no false positives out of the total 11925 samples. The precision, recall, and F1-score are consistently high for both classes, indicating a well-balanced model capable of effectively identifying instances of both categories. The macro average and weighted average for precision, recall, and F1-score are also 1.00, further confirming the model's robustness. Both Random Forest and XGBoost demonstrate exceptional performance with validation accuracy, precision, recall, and F1-score close to 1.00, indicating near-perfect classification capabilities for the two-class problem. Both models exhibit minimal misclassifications, with very low false negatives and no false positives. The macro and weighted averages for precision, recall, and F1-score are consistently high for both algorithms, highlighting their balanced performance.

In the next step we consider all the classes from the given dataset. The Random Forest model achieved a respectable overall accuracy of approximately 0.90 in classifying the dataset with 17 classes, where one class represents benign samples, and the rest are subcategories of malicious samples. The model demonstrated varying performance across the different classes, with precision, recall, and F1-scores ranging from 0.54 to 1.00. It notably excelled in classifying class 2, achieving perfect precision, recall, and F1-score, indicating accurate identification of this category. However, it faced challenges in classifying class 1, achieving lower precision, recall, and F1-score, possibly due to the imbalance in class sizes or the complexity of distinguishing the subcategory. Overall, the Random Forest model provides a promising performance, but further analysis and tuning may be required to improve its accuracy on specific classes with relatively lower scores. The XGBoost model also exhibits a strong performance with an overall accuracy of approximately 0.90 for the dataset containing 17 classes, where one class represents benign samples and the others are subcategories of malicious samples. Similar to the Random Forest model, XGBoost demonstrates varying performance across the different classes, with precision, recall, and F1-scores ranging from 0.55 to 1.00. It performs exceptionally well in classifying class 2, achieving perfect precision, recall, and F1-score, indicating accurate identification of this category. However, it faces challenges in classifying class 4 and class 13, with relatively lower precision, recall, and F1-scores, suggesting the difficulty in distinguishing these subcategories from others.

Both the Random Forest and XGBoost models demonstrate strong performance in classifying the dataset with 17 classes, where one class represents benign samples and the others are subcategories of malicious samples. Both models achieve a similar overall accuracy of approximately 0.90, indicating their ability to make correct predictions on a majority of instances.

In terms of precision, recall, and F1-scores, both models show variations across different classes. Random Forest performs slightly better in classifying class 4, achieving higher precision, recall, and F1-score compared to XGBoost. On the other hand, XGBoost outperforms Random Forest in classifying class 1, achieving higher precision, recall, and F1-score.


"""